{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexfazio/firecrawl-quickstart/blob/main/claude_researcher_with_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f015733",
      "metadata": {
        "id": "8f015733"
      },
      "source": [
        "# Firecrawl Web Crawling with OpenAI and Anthropic\n",
        "This notebook demonstrates how to use the Firecrawl API along with OpenAI's Anthropic to search for specific information on a website. It takes a user-defined objective and website URL, then attempts to find relevant pages and extract information based on the objective.\n",
        "\n",
        "### Requirements\n",
        "1. **Firecrawl API key**: Obtain from your Firecrawl account.\n",
        "2. **Anthropic API key**: Obtain from Anthropic if you're leveraging their models.\n",
        "3. **AgentOps API key**: If using AgentOps, include its API key.\n",
        "\n",
        "Set up your API keys as environment variables or directly in the notebook for ease of access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "TuuO7HAFyuq9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuuO7HAFyuq9",
        "outputId": "eb05b163-5401-4728-82d7-53027821df2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/946.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m942.1/946.0 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m946.0/946.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/288.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q firecrawl-py anthropic agentops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8e2be400",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2be400",
        "outputId": "6e6ac3a6-7be0-4b46-e791-46873d7c2542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FIRECRAWL_API_KEY··········\n",
            "ANTHROPIC_API_KEY··········\n",
            "AGENTOPS_API_KEY··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "import re\n",
        "from firecrawl import FirecrawlApp\n",
        "import json\n",
        "import anthropic\n",
        "import agentops\n",
        "\n",
        "# Initialize the FirecrawlApp and OpenAI client\n",
        "app = FirecrawlApp(api_key=getpass('FIRECRAWL_API_KEY'))\n",
        "client = anthropic.Anthropic(api_key=getpass('ANTHROPIC_API_KEY'))\n",
        "AGENTOPS_API_KEY = getpass('AGENTOPS_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c625af64",
      "metadata": {
        "id": "c625af64"
      },
      "source": [
        "### ANSI Color Codes\n",
        "For adding colored output in the notebook, we define a class for color codes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36be5503",
      "metadata": {
        "id": "36be5503"
      },
      "outputs": [],
      "source": [
        "class Colors:\n",
        "    CYAN = '\\033[96m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    GREEN = '\\033[92m'\n",
        "    RED = '\\033[91m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    BLUE = '\\033[94m'\n",
        "    RESET = '\\033[0m'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e732d54d",
      "metadata": {
        "id": "e732d54d"
      },
      "source": [
        "### Step 1: Finding the Relevant Page\n",
        "The function `find_relevant_page_via_map` takes an objective and a website URL. It then uses the Anthropic client to generate search parameters for the Firecrawl API to map the website and identify relevant pages based on the objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "13f88d4a",
      "metadata": {
        "id": "13f88d4a"
      },
      "outputs": [],
      "source": [
        "def find_relevant_page_via_map(objective, url, app, client):\n",
        "    try:\n",
        "        print(f\"{Colors.CYAN}Objective: {objective}{Colors.RESET}\")\n",
        "        print(f\"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}\")\n",
        "\n",
        "        map_prompt = f\"\"\"\n",
        "        The map function generates a list of URLs from a website and accepts a search parameter.\n",
        "        Based on the objective of: {objective}, suggest a 1-2 word search parameter.\n",
        "        \"\"\"\n",
        "\n",
        "        completion = client.messages.create(\n",
        "            model='claude-3-5-sonnet-20241022',\n",
        "            max_tokens=1000,\n",
        "            temperature=0,\n",
        "            system=\"Expert web crawler\",\n",
        "            messages=[{'role': 'user', 'content': map_prompt}]\n",
        "        )\n",
        "\n",
        "        map_search_parameter = completion.content[0].text\n",
        "        map_website = app.map_url(url, params={'search': map_search_parameter})\n",
        "\n",
        "        print(f\"{Colors.GREEN}Mapping completed. Links found: {len(map_website['links'])}{Colors.RESET}\")\n",
        "        return map_website['links']\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error: {str(e)}{Colors.RESET}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49f669d",
      "metadata": {
        "id": "c49f669d"
      },
      "source": [
        "### Step 2: Analyzing Top Pages\n",
        "The function `find_objective_in_top_pages` examines the top pages from the website map, attempting to fulfill the user's objective using scraped content. If the objective is met, it returns the relevant data in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "812fd739",
      "metadata": {
        "id": "812fd739"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_objective_in_top_pages(map_website, objective, app, client):\n",
        "    try:\n",
        "        top_links = map_website[:2]\n",
        "        print(f\"{Colors.CYAN}Analyzing top links: {top_links}{Colors.RESET}\")\n",
        "\n",
        "        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})\n",
        "        print(f\"{Colors.GREEN}Batch scraping completed.{Colors.RESET}\")\n",
        "\n",
        "        for scrape_result in batch_scrape_result['data']:\n",
        "            check_prompt = f\"\"\"\n",
        "            Given scraped content and objective, determine if the objective is met.\n",
        "            Extract relevant information in simple JSON if met.\n",
        "            Objective: {objective}\n",
        "            Scraped content: {scrape_result['markdown']}\n",
        "            \"\"\"\n",
        "\n",
        "            completion = client.messages.create(\n",
        "                model='claude-3-5-sonnet-20241022',\n",
        "                max_tokens=1000,\n",
        "                temperature=0,\n",
        "                system=\"Expert web crawler\",\n",
        "                messages=[{'role': 'user', 'content': check_prompt}]\n",
        "            )\n",
        "\n",
        "            result = completion.content[0].text\n",
        "            if result and result != 'Objective not met':\n",
        "                try:\n",
        "                    return json.loads(result)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"{Colors.RED}JSON parsing error: {e}. Raw result: {result}{Colors.RESET}\")\n",
        "                    continue  # Skip to the next result if parsing fails\n",
        "\n",
        "        print(f\"{Colors.RED}Objective not met in examined content.{Colors.RESET}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error during analysis: {str(e)}{Colors.RESET}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc4cef6",
      "metadata": {
        "id": "9fc4cef6"
      },
      "source": [
        "### Step 3: Executing the Main Function\n",
        "The main function prompts for user input (website URL and objective), calls the `find_relevant_page_via_map` and `find_objective_in_top_pages` functions, and displays results accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e3721623",
      "metadata": {
        "id": "e3721623"
      },
      "outputs": [],
      "source": [
        "# Flag to track if AgentOps has been initialized\n",
        "AGENTOPS_INITIALIZED = False\n",
        "\n",
        "def main():\n",
        "    url = input(f\"{Colors.BLUE}Enter website URL:{Colors.RESET}\") or \"https://www.firecrawl.dev/\"\n",
        "    objective = input(f\"{Colors.BLUE}Enter objective:{Colors.RESET}\") or \"find pricing plans\"\n",
        "\n",
        "    map_website = find_relevant_page_via_map(objective, url, app, client)\n",
        "\n",
        "    if map_website:\n",
        "        result = find_objective_in_top_pages(map_website, objective, app, client)\n",
        "        if result:\n",
        "            print(f\"{Colors.GREEN}Objective met. Extracted info:{Colors.RESET}\")\n",
        "            print(f\"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Colors.RED}Objective not fulfilled with available content.{Colors.RESET}\")\n",
        "    else:\n",
        "        print(f\"{Colors.RED}No relevant pages identified.{Colors.RESET}\")\n",
        "\n",
        "def find_objective_in_top_pages(map_website, objective, app, client):\n",
        "    try:\n",
        "        top_links = map_website[:2]\n",
        "        print(f\"{Colors.CYAN}Analyzing top links: {top_links}{Colors.RESET}\")\n",
        "\n",
        "        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})\n",
        "        print(f\"{Colors.GREEN}Batch scraping completed.{Colors.RESET}\")\n",
        "\n",
        "        # Regex pattern to match JSON-like blocks in the response\n",
        "        json_pattern = r\"\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}\"\n",
        "\n",
        "        for scrape_result in batch_scrape_result['data']:\n",
        "            check_prompt = f\"\"\"\n",
        "            Given scraped content and objective, determine if the objective is met.\n",
        "            Extract relevant information in simple JSON if met.\n",
        "            Objective: {objective}\n",
        "            Scraped content: {scrape_result['markdown']}\n",
        "            \"\"\"\n",
        "\n",
        "            completion = client.messages.create(\n",
        "                model='claude-3-5-sonnet-20241022',\n",
        "                max_tokens=1000,\n",
        "                temperature=0,\n",
        "                system=\"Expert web crawler\",\n",
        "                messages=[{'role': 'user', 'content': check_prompt}]\n",
        "            )\n",
        "\n",
        "            result = completion.content[0].text\n",
        "            # Search for JSON-like block in the result text\n",
        "            json_match = re.search(json_pattern, result, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    return json.loads(json_match.group(0))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"{Colors.RED}JSON parsing error: {e}. Raw result: {json_match.group(0)}{Colors.RESET}\")\n",
        "                    continue  # Skip to the next result if parsing fails\n",
        "            else:\n",
        "                print(f\"{Colors.YELLOW}No JSON found in the response. Raw result: {result}{Colors.RESET}\")\n",
        "\n",
        "        print(f\"{Colors.RED}Objective not met in examined content.{Colors.RESET}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error during analysis: {str(e)}{Colors.RESET}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0cr6H3nlBSMG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cr6H3nlBSMG",
        "outputId": "fd547965-e708-43e4-d4e8-36b743dd247b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mEnter website URL:\u001b[0mhttps://www.firecrawl.dev/\n",
            "\u001b[94mEnter objective:\u001b[0mfind me the pricing plans\n",
            "\u001b[96mObjective: find me the pricing plans\u001b[0m\n",
            "\u001b[96mInitiating search on the website: https://www.firecrawl.dev/\u001b[0m\n",
            "\u001b[92mMapping completed. Links found: 39\u001b[0m\n",
            "\u001b[96mAnalyzing top links: ['https://www.firecrawl.dev/pricing', 'https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl']\u001b[0m\n",
            "\u001b[92mBatch scraping completed.\u001b[0m\n",
            "\u001b[92mObjective met. Extracted info:\u001b[0m\n",
            "\u001b[95m{\n",
            "  \"name\": \"Free Plan\",\n",
            "  \"price\": \"$0\",\n",
            "  \"credits\": \"500 credits\",\n",
            "  \"features\": [\n",
            "    \"Scrape 500 pages\",\n",
            "    \"10 /scrape per min\",\n",
            "    \"1 /crawl per min\"\n",
            "  ]\n",
            "}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}