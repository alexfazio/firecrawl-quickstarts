{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexfazio/firecrawl-quickstarts/blob/main/claude_researcher_with_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f015733",
      "metadata": {
        "id": "8f015733"
      },
      "source": [
        "# Firecrawl Web Crawling with OpenAI and Anthropic\n",
        "This notebook demonstrates how to use the Firecrawl API along with OpenAI's Anthropic to search for specific information on a website. It takes a user-defined objective and website URL, then attempts to find relevant pages and extract information based on the objective.\n",
        "\n",
        "### Requirements\n",
        "1. **Firecrawl API key**: Obtain from your Firecrawl account.\n",
        "2. **Anthropic API key**: Obtain from Anthropic if you're leveraging their models.\n",
        "3. **AgentOps API key**: If using AgentOps, include its API key.\n",
        "\n",
        "Set up your API keys as environment variables or directly in the notebook for ease of access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "TuuO7HAFyuq9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuuO7HAFyuq9",
        "outputId": "883e1c91-b16e-4296-90eb-d25fef50bd4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/946.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m946.0/946.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/288.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q firecrawl-py anthropic agentops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8e2be400",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2be400",
        "outputId": "277f7065-f8db-464e-9f39-4480453e645a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FIRECRAWL_API_KEY··········\n",
            "ANTHROPIC_API_KEY··········\n",
            "AGENTOPS_API_KEY··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "from firecrawl import FirecrawlApp\n",
        "import os, re, json, anthropic, agentops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the FirecrawlApp, OpenAI client, and AgentOps\n",
        "app = FirecrawlApp(api_key=getpass('FIRECRAWL_API_KEY'))\n",
        "client = anthropic.Anthropic(api_key=getpass('ANTHROPIC_API_KEY'))\n",
        "AGENTOPS_API_KEY = getpass('AGENTOPS_API_KEY')"
      ],
      "metadata": {
        "id": "RJZQ-gYpGOl9"
      },
      "id": "RJZQ-gYpGOl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c625af64",
      "metadata": {
        "id": "c625af64"
      },
      "source": [
        "### ANSI Color Codes\n",
        "For adding colored output in the notebook, we define a class for color codes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "36be5503",
      "metadata": {
        "id": "36be5503"
      },
      "outputs": [],
      "source": [
        "class Colors:\n",
        "    CYAN = '\\033[96m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    GREEN = '\\033[92m'\n",
        "    RED = '\\033[91m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    BLUE = '\\033[94m'\n",
        "    RESET = '\\033[0m'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e732d54d",
      "metadata": {
        "id": "e732d54d"
      },
      "source": [
        "### Step 1: Finding the Relevant Page\n",
        "The function `find_relevant_page_via_map` takes an objective and a website URL. It then uses the Anthropic client to generate search parameters for the Firecrawl API to map the website and identify relevant pages based on the objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "13f88d4a",
      "metadata": {
        "id": "13f88d4a"
      },
      "outputs": [],
      "source": [
        "def find_relevant_page_via_map(objective, url, app, client):\n",
        "    try:\n",
        "        print(f\"{Colors.CYAN}Objective: {objective}{Colors.RESET}\")\n",
        "        print(f\"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}\")\n",
        "\n",
        "        map_prompt = f\"\"\"\n",
        "        The map function generates a list of URLs from a website and accepts a search parameter.\n",
        "        Based on the objective of: {objective}, suggest a 1-2 word search parameter.\n",
        "        \"\"\"\n",
        "\n",
        "        completion = client.messages.create(\n",
        "            model='claude-3-5-sonnet-20241022',\n",
        "            max_tokens=1000,\n",
        "            temperature=0,\n",
        "            system=\"Expert web crawler\",\n",
        "            messages=[{'role': 'user', 'content': map_prompt}]\n",
        "        )\n",
        "\n",
        "        map_search_parameter = completion.content[0].text\n",
        "        map_website = app.map_url(url, params={'search': map_search_parameter})\n",
        "\n",
        "        print(f\"{Colors.GREEN}Mapping completed. Links found: {len(map_website['links'])}{Colors.RESET}\")\n",
        "        return map_website['links']\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error: {str(e)}{Colors.RESET}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49f669d",
      "metadata": {
        "id": "c49f669d"
      },
      "source": [
        "### Step 2: Examining Top Pages using Firewcrawl's [Map](https://docs.firecrawl.dev/features/map)\n",
        "The function `find_objective_in_top_pages` examines the top pages from the website map, attempting to fulfill the user's objective using scraped content. If the objective is met, it returns the relevant data in JSON format.\n",
        "\n",
        "**Note:** Firecrawl's Map Response will be an ordered list from the most relevant to the least relevant. By selecting only the first three elements (`[:3]`), the function focuses on analyzing just the top three most relevant pages identified during the mapping stage..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "812fd739",
      "metadata": {
        "id": "812fd739"
      },
      "outputs": [],
      "source": [
        "def find_objective_in_top_pages(map_website, objective, app, client):\n",
        "    try:\n",
        "        # Get top 2 links from the map result\n",
        "        top_links = map_website[:3]\n",
        "        print(f\"{Colors.CYAN}Analyzing the {len(top_links)} top links: {top_links}{Colors.RESET}\")\n",
        "\n",
        "        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})\n",
        "        print(f\"{Colors.GREEN}Batch scraping completed.{Colors.RESET}\")\n",
        "\n",
        "        for scrape_result in batch_scrape_result['data']:\n",
        "            check_prompt = f\"\"\"\n",
        "            Given scraped content and objective, determine if the objective is met.\n",
        "            Extract relevant information in simple JSON if met.\n",
        "            Objective: {objective}\n",
        "            Scraped content: {scrape_result['markdown']}\n",
        "            \"\"\"\n",
        "\n",
        "            completion = client.messages.create(\n",
        "                model='claude-3-5-sonnet-20241022',\n",
        "                max_tokens=1000,\n",
        "                temperature=0,\n",
        "                system=\"Expert web crawler\",\n",
        "                messages=[{'role': 'user', 'content': check_prompt}]\n",
        "            )\n",
        "\n",
        "            result = completion.content[0].text\n",
        "            if result and result != 'Objective not met':\n",
        "                try:\n",
        "                    return json.loads(result)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"{Colors.RED}JSON parsing error: {e}. Raw result: {result}{Colors.RESET}\")\n",
        "                    continue  # Skip to the next result if parsing fails\n",
        "\n",
        "        print(f\"{Colors.RED}Objective not met in examined content.{Colors.RESET}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error during analysis: {str(e)}{Colors.RESET}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Find and Extract Information\n",
        "\n",
        "This function aims to find and extract information related to a given `objective` from the top-ranked pages of a website.\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "1. **Selects Top Links:** It selects the top two URLs from the `map_website` list, assuming they are the most relevant to the objective.\n",
        "2. **Scrapes Content:** It uses the `app.batch_scrape_urls` function to scrape content from these selected URLs in Markdown format.\n",
        "3. **Analyzes Content:**  For each scraped page, it constructs a prompt for the Anthropic Claude model. This prompt asks the model to determine if the scraped content fulfills the `objective`. If it does, the model is asked to extract the relevant information and format it as JSON.\n",
        "4. **Extracts JSON:** The function uses a regular expression to identify JSON-like blocks within the Anth"
      ],
      "metadata": {
        "id": "l3WzoekQEFCq"
      },
      "id": "l3WzoekQEFCq"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kc31wkA3EEtl"
      },
      "id": "Kc31wkA3EEtl"
    },
    {
      "cell_type": "code",
      "source": [
        "def find_objective_in_top_pages(map_website, objective, app, client):\n",
        "    try:\n",
        "        top_links = map_website[:2]\n",
        "        print(f\"{Colors.CYAN}Analyzing top links: {top_links}{Colors.RESET}\")\n",
        "\n",
        "        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})\n",
        "        print(f\"{Colors.GREEN}Batch scraping completed.{Colors.RESET}\")\n",
        "\n",
        "        # Regex pattern to match JSON-like blocks in the response\n",
        "        json_pattern = r\"\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}\"\n",
        "\n",
        "        for scrape_result in batch_scrape_result['data']:\n",
        "            check_prompt = f\"\"\"\n",
        "            Given scraped content and objective, determine if the objective is met.\n",
        "            Extract relevant information in simple JSON if met.\n",
        "            Objective: {objective}\n",
        "            Scraped content: {scrape_result['markdown']}\n",
        "            \"\"\"\n",
        "\n",
        "            completion = client.messages.create(\n",
        "                model='claude-3-5-sonnet-20241022',\n",
        "                max_tokens=1000,\n",
        "                temperature=0,\n",
        "                system=\"Expert web crawler\",\n",
        "                messages=[{'role': 'user', 'content': check_prompt}]\n",
        "            )\n",
        "\n",
        "            result = completion.content[0].text\n",
        "            # Search for JSON-like block in the result text\n",
        "            json_match = re.search(json_pattern, result, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    return json.loads(json_match.group(0))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"{Colors.RED}JSON parsing error: {e}. Raw result: {json_match.group(0)}{Colors.RESET}\")\n",
        "                    continue  # Skip to the next result if parsing fails\n",
        "            else:\n",
        "                print(f\"{Colors.YELLOW}No JSON found in the response. Raw result: {result}{Colors.RESET}\")\n",
        "\n",
        "        print(f\"{Colors.RED}Objective not met in examined content.{Colors.RESET}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error during analysis: {str(e)}{Colors.RESET}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "02DvK01wELut"
      },
      "id": "02DvK01wELut",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9fc4cef6",
      "metadata": {
        "id": "9fc4cef6"
      },
      "source": [
        "### Step 4: Executing the Main Function\n",
        "The main function prompts for user input (website URL and objective), calls the `find_relevant_page_via_map` and `find_objective_in_top_pages` functions, and displays results accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e3721623",
      "metadata": {
        "id": "e3721623"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    url = input(f\"{Colors.BLUE}Enter website URL:{Colors.RESET}\") or \"https://www.firecrawl.dev/\"\n",
        "    objective = input(f\"{Colors.BLUE}Enter objective:{Colors.RESET}\") or \"find pricing plans\"\n",
        "\n",
        "    map_website = find_relevant_page_via_map(objective, url, app, client)\n",
        "\n",
        "    if map_website:\n",
        "        result = find_objective_in_top_pages(map_website, objective, app, client)\n",
        "        if result:\n",
        "            print(f\"{Colors.GREEN}Objective met. Extracted info:{Colors.RESET}\")\n",
        "            print(f\"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Colors.RED}Objective not fulfilled with available content.{Colors.RESET}\")\n",
        "    else:\n",
        "        print(f\"{Colors.RED}No relevant pages identified.{Colors.RESET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0cr6H3nlBSMG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cr6H3nlBSMG",
        "outputId": "d37dda57-fe49-4346-c748-3c8cc557bebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mEnter website URL:\u001b[0mhttps://www.firecrawl.dev/\n",
            "\u001b[94mEnter objective:\u001b[0myes or no: is firecrawl backed by y combinator?\n",
            "\u001b[96mObjective: yes or no: is firecrawl backed by y combinator?\u001b[0m\n",
            "\u001b[96mInitiating search on the website: https://www.firecrawl.dev/\u001b[0m\n",
            "'\\x1b[92mMapping completed. Links found: 42\\x1b[0m'\n",
            "\u001b[96mAnalyzing top links: ['https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned', 'https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python']\u001b[0m\n",
            "\u001b[92mBatch scraping completed.\u001b[0m\n",
            "\u001b[92mObjective met. Extracted info:\u001b[0m\n",
            "\u001b[95m{\n",
            "  \"can_determine\": false,\n",
            "  \"reason\": \"No mention of Y Combinator backing in the provided content\"\n",
            "}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}