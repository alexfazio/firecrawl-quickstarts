{
    "https://huggingface.co/papers/2412.14161": {
        "paper_title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World  Tasks",
        "number_of_upvotes": 24,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14161",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14161",
        "authors": "Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, Graham Neubig",
        "abstract_body": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.13795": {
        "paper_title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN",
        "number_of_upvotes": 6,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13795",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13795",
        "authors": "Pengxiang Li, Lu Yin, Shiwei Liu",
        "abstract_body": "Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network--both shallow and deep layers--to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14173": {
        "paper_title": "AniDoc: Animation Creation Made Easier",
        "number_of_upvotes": 21,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14173",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14173",
        "authors": "Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, Huamin Qu",
        "abstract_body": "The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: https://yihao-meng.github.io/AniDoc_demo.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.12953": {
        "paper_title": "Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning",
        "number_of_upvotes": 8,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.12953",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.12953",
        "authors": "Moritz Reuss, Jyothish Pari, Pulkit Agrawal, Rudolf Lioutikov",
        "abstract_body": "Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior. As models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling laws. Therefore, continuing with the current architectures will present a computational roadblock. To address this gap, we propose Mixture-of-Denoising Experts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling through sparse experts and noise-conditioned routing, reducing both active parameters by 40% and inference costs by 90% via expert caching. Our architecture combines this efficient scaling with noise-conditioned self-attention mechanism, enabling more effective denoising across different noise levels. MoDE achieves state-of-the-art performance on 134 tasks in four established imitation learning benchmarks (CALVIN and LIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01 on CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and Transformer Diffusion Policies by an average of 57% across 4 benchmarks, while using 90% fewer FLOPs and fewer active parameters compared to default Diffusion Transformer architectures. Furthermore, we conduct comprehensive ablations on MoDE's components, providing insights for designing efficient and scalable Transformer architectures for Diffusion Policies. Code and demonstrations are available at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
        "utc_publication_date_day": 17,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14123": {
        "paper_title": "AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities",
        "number_of_upvotes": 6,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14123",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14123",
        "authors": "Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu",
        "abstract_body": "Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and 11 distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned, we achieve better or near state-of-the-art results on the datasets of GeoPlex and 4 additional ones for 5 environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, and flood segmentation. The code and models are available at https://github.com/gastruc/AnySat.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14015": {
        "paper_title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth  Estimation",
        "number_of_upvotes": 8,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14015",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14015",
        "authors": "Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang",
        "abstract_body": "Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. Our approach sets new state-of-the-arts on the ARKitScenes and ScanNet++ datasets and benefits downstream applications, including 3D reconstruction and generalized robotic grasping.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.13501": {
        "paper_title": "GUI Agents: A Survey",
        "number_of_upvotes": 7,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13501",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13501",
        "authors": "Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, Seunghyun Yoon",
        "abstract_body": "Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.",
        "utc_publication_date_day": 17,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.13871": {
        "paper_title": "LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via  Hierarchical Window Transformer",
        "number_of_upvotes": 4,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13871",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13871",
        "authors": "Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun",
        "abstract_body": "In multimodal large language models (MLLMs), vision transformers (ViTs) are widely employed for visual encoding. However, their performance in solving universal MLLM tasks is not satisfactory. We attribute it to a lack of information from diverse visual levels, impeding alignment with the various semantic granularity required for language generation. To address this issue, we present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window transformer that enables capturing diverse visual granularity by constructing and integrating a high-resolution feature pyramid. As a vision-language projector, Hiwin transformer comprises two primary modules: (i) an inverse feature pyramid, constructed by a ViT-derived feature up-sampling process utilizing high-frequency details from an image pyramid, and (ii) hierarchical window attention, focusing on a set of key sampling features within cross-scale windows to condense multi-level feature maps. Extensive experiments demonstrate that LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular benchmarks. Notably, our design brings an average boost of 3.7% across 14 benchmarks compared with the baseline method, 9.3% on DocVQA for instance. We make all the data, model checkpoint, and code publicly available to facilitate future research.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2023,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2023
    },
    "https://huggingface.co/papers/2412.14168": {
        "paper_title": "FashionComposer: Compositional Fashion Image Generation",
        "number_of_upvotes": 11,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14168",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14168",
        "authors": "Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao",
        "abstract_body": "We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input (i.e., text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model's robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an 'asset library' and employ a reference UNet to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different 'assets' with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.13061": {
        "paper_title": "VidTok: A Versatile and Open-Source Video Tokenizer",
        "number_of_upvotes": 3,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13061",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13061",
        "authors": "Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, Jiang Bian",
        "abstract_body": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.",
        "utc_publication_date_day": 17,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.13670": {
        "paper_title": "AntiLeak-Bench: Preventing Data Contamination by Automatically  Constructing Benchmarks with Updated Real-World Knowledge",
        "number_of_upvotes": 2,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13670",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13670",
        "authors": "Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu, William Yang Wang",
        "abstract_body": "Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2023,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2023
    },
    "https://huggingface.co/papers/2412.12571": {
        "paper_title": "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers",
        "number_of_upvotes": 3,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.12571",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.12571",
        "authors": "Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Chen Liang, Tong Shen, Han Zhang, Huanzhang Dou, Yu Liu, Jingren Zhou",
        "abstract_body": "Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, a zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs a multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, a Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench arXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at https://github.com/ali-vilab/ChatDiT",
        "utc_publication_date_day": 17,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2023,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2023
    },
    "https://huggingface.co/papers/2412.13303": {
        "paper_title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
        "number_of_upvotes": 0,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13303",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13303",
        "authors": "Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari",
        "abstract_body": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2times improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152times1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision encoder that is 3.4times smaller.",
        "utc_publication_date_day": 17,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14093": {
        "paper_title": "Alignment faking in large language models",
        "number_of_upvotes": 1,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14093",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14093",
        "authors": "Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, SÃ¶ren Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Evan Hubinger",
        "abstract_body": "We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14172": {
        "paper_title": "Learning from Massive Human Videos for Universal Humanoid Pose Control",
        "number_of_upvotes": 4,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14172",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14172",
        "authors": "Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, Yue Wang",
        "abstract_body": "Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2412,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2412
    },
    "https://huggingface.co/papers/2412.13746": {
        "paper_title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented  Generation for Preference Alignment",
        "number_of_upvotes": 6,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.13746",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.13746",
        "authors": "Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",
        "abstract_body": "Despite the significant progress made by existing retrieval augmented language models (RALMs) in providing trustworthy responses and grounding in reliable sources, they often overlook effective alignment with human preferences. In the alignment process, reward models (RMs) act as a crucial proxy for human values to guide optimization. However, it remains unclear how to evaluate and select a reliable RM for preference alignment in RALMs. To this end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG settings. First, we design four crucial and challenging RAG-specific scenarios to assess RMs, including multi-hop reasoning, fine-grained citation, appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG subsets, six retrievers, and 24 RALMs to increase the diversity of data sources. Finally, we adopt an LLM-as-a-judge approach to improve preference annotation efficiency and effectiveness, exhibiting a strong correlation with human annotations. Based on the RAG-RewardBench, we conduct a comprehensive evaluation of 45 RMs and uncover their limitations in RAG scenarios. Additionally, we also reveal that existing trained RALMs show almost no improvement in preference alignment, highlighting the need for a shift towards preference-aligned training. We release our benchmark and code publicly at https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14042": {
        "paper_title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
        "number_of_upvotes": 2,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14042",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14042",
        "authors": "Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, Anis Kacem, Djamila Aouada",
        "abstract_body": "Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained solely on a proposed synthetic dataset of one million diverse CAD sequences. CAD-Recode significantly outperforms existing methods across three datasets while requiring fewer input points. Notably, it achieves 10 times lower mean Chamfer distance than state-of-the-art methods on DeepCAD and Fusion360 datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2024,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2024
    },
    "https://huggingface.co/papers/2412.14171": {
        "paper_title": "Thinking in Space: How Multimodal Large Language Models See, Remember,  and Recall Spaces",
        "number_of_upvotes": 2,
        "number_of_comments": 0,
        "view_pdf_url": "https://arxiv.org/pdf/2412.14171",
        "view_arxiv_page_url": "https://arxiv.org/abs/2412.14171",
        "authors": "Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, Saining Xie",
        "abstract_body": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also \"think in space\" from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance ability.",
        "utc_publication_date_day": 18,
        "utc_publication_date_month": 12,
        "utc_publication_date_year": 2023,
        "utc_submission_date_day": 19,
        "utc_submission_date_month": 12,
        "utc_submission_date_year": 2023
    }
}